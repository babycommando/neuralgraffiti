{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAm95GH7fHhC"
      },
      "outputs": [],
      "source": [
        "# Neural Graffiti Layer Injected Directly into Gemma (no retraining, live adapter mod)\n",
        "\n",
        "!pip install transformers torch accelerate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Hugging Face Token\n",
        "hf_token = \"\"\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\", token=hf_token)\n",
        "\n",
        "# Load Base Model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-3-1b-it\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ").eval()\n",
        "\n",
        "# Neural Graffiti Spray Layer\n",
        "# ---------------------------\n",
        "# The SprayLayer is a simple dynamic state mechanism inspired by (but not replicating) liquid neural networks.\n",
        "# It maintains a persistent internal state vector that is updated over time based on new input embeddings.\n",
        "# The update rule is a form of neural feedback:\n",
        "#     dx = -Œª * (state - W(x))\n",
        "# This allows the layer to evolve with each input, retaining a trace of prior interactions.\n",
        "# The resulting \"spray vector\" reflects the model's internal memory drift and is used to modulate\n",
        "# the hidden activations of the base transformer, injecting a sense of continuity and influence\n",
        "# across otherwise disconnected prompts ‚Äî like tagging each thought with memory residue.\n",
        "class SprayLayer(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(dim, dim)\n",
        "        self.lambda_ = nn.Parameter(torch.ones(dim) * 0.1)\n",
        "        self.register_buffer('state', torch.zeros(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        dx = -self.lambda_ * (self.state - self.W(x))\n",
        "        self.state = self.state + dx\n",
        "        return self.state\n",
        "\n",
        "# Graffiti Adapter Module\n",
        "class GraffitiAdapter(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.spray = SprayLayer(hidden_size).half()\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.2).half())\n",
        "\n",
        "    def forward(self, hidden, memory_embed):\n",
        "        spray_vector = self.spray(memory_embed)\n",
        "        return hidden + self.alpha * spray_vector.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "# Memory Functions\n",
        "memory_bank = []\n",
        "\n",
        "def store_memory(embedding, text):\n",
        "    memory_bank.append((embedding.detach().clone(), text))\n",
        "\n",
        "def recall_memory(query_embedding, top_k=3):\n",
        "    if not memory_bank:\n",
        "        return []\n",
        "    similarities = [\n",
        "        (F.cosine_similarity(query_embedding, mem[0], dim=0), mem[0])\n",
        "        for mem in memory_bank\n",
        "    ]\n",
        "    return [mem for _, mem in sorted(similarities, reverse=True)[:top_k]]\n",
        "\n",
        "def fuse_embeddings(current, recalled):\n",
        "    if recalled:\n",
        "        vectors = [current] + recalled\n",
        "        return torch.mean(torch.stack(vectors), dim=0)\n",
        "    return current\n",
        "\n",
        "# Wrapped Gemma Model with Graffiti Adapter\n",
        "class GraffitiWrappedModel(nn.Module):\n",
        "    def __init__(self, base_model, graffiti_adapter):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.graffiti_adapter = graffiti_adapter\n",
        "\n",
        "    def forward(self, input_ids, memory_embed=None, **kwargs):\n",
        "        outputs = self.base_model.model(\n",
        "            input_ids=input_ids,\n",
        "            output_hidden_states=True,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        hidden_states = outputs.hidden_states[-1]\n",
        "\n",
        "        if memory_embed is not None:\n",
        "            hidden_states = self.graffiti_adapter(hidden_states, memory_embed)\n",
        "\n",
        "        logits = self.base_model.lm_head(hidden_states)\n",
        "        return logits\n",
        "\n",
        "# Initialize Adapter and Model\n",
        "graffiti_adapter = GraffitiAdapter(hidden_size=base_model.config.hidden_size).to(base_model.device)\n",
        "model = GraffitiWrappedModel(base_model, graffiti_adapter).eval()\n",
        "\n",
        "# Graffiti Text Generator\n",
        "@torch.no_grad()\n",
        "def graffiti_generate(user_input, conversation_history=\"\", max_new_tokens=100):\n",
        "    prompt = f\"{conversation_history}<start_of_turn>user\\n{user_input}\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "    outputs = base_model.model(**inputs, output_hidden_states=True)\n",
        "    hidden = outputs.hidden_states[-1].squeeze(0)\n",
        "    sentence_embedding = torch.mean(hidden, dim=0)\n",
        "    recalled = recall_memory(sentence_embedding)\n",
        "    fused = fuse_embeddings(sentence_embedding, recalled)\n",
        "    spray_vector = graffiti_adapter.spray(fused)\n",
        "    store_memory(spray_vector, user_input)\n",
        "\n",
        "    print(\"\\nüß† Neural Graffiti injected into model.\")\n",
        "    print(f\"   ‚Ä¢ Recalled memories: {len(recalled)}\")\n",
        "    print(f\"   ‚Ä¢ Spray state: {graffiti_adapter.spray.state.mean().item():.4f}\")\n",
        "\n",
        "    logits = model(input_ids=inputs.input_ids, memory_embed=spray_vector)\n",
        "    next_tokens = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)\n",
        "\n",
        "    generated = torch.cat([inputs.input_ids, next_tokens], dim=1)\n",
        "    for _ in range(max_new_tokens - 1):\n",
        "        logits = model(input_ids=generated, memory_embed=spray_vector)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "        generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "    return tokenizer.decode(generated[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "# Graffiti Chat Loop\n",
        "def graffiti_chat():\n",
        "    print(\"\\nüîÅ Graffiti-Gemma Chat (Neural Graffiti mode) ‚Äî type 'quit' to exit\\n\")\n",
        "    conversation_history = \"\"\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower().strip() == 'quit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        response = graffiti_generate(user_input, conversation_history)\n",
        "        print(f\"Graffiti-Gemma: {response}\\n\")\n",
        "\n",
        "        conversation_history += f\"<start_of_turn>user\\n{user_input}\\n<end_of_turn>\\n<start_of_turn>model\\n{response}\\n<end_of_turn>\\n\"\n",
        "\n",
        "graffiti_chat()\n"
      ]
    }
  ]
}
